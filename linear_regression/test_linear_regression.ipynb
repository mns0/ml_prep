{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going Back-to-Basics\n",
    "## Linear methods for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from linear_regression import linear_regression  as lr \n",
    "from sklearn import datasets\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this series of back-to-basic posts I'm reviews some simpler but powerful inferential statistical methods. Here we're focusing on the good ol'fashion linear regression. In the next post I'll dive into some shrinkage techniques but for now we're keeping it simple. The last cell contains all the code in the linear regression class.\n",
    "\n",
    "I'm using the [prostate data](https://web.stanford.edu/~hastie/ElemStatLearn/data.html) set following my favorite machine learning text, [The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regressions is the minimization of a cost function $E[(Y-\\hat{Y})^2]$ where $\\hat{Y} = X\\beta$ where $\\beta$ are the estimated regression coefficients and $X$~is a $(p+1)\\times~N$ matrix containing p predictors and N data points. The expectation is taken empirically.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The regression is calculated by optimizing the coefficients, $\\hat{\\beta}$ to minimize the cost (RSS),\n",
    "\n",
    "\n",
    "$$\n",
    "RSS = (Y-\\beta X)^T(Y-\\beta X)\\\\\n",
    "= Y^TY  - Y\\beta X - YX^T\\beta^T +X^T\\beta^T\\beta X,\\\\\n",
    "\\nabla_{\\beta} RSS = 0  \\\\\n",
    "= -2X^TY  + 2 X^TX\\beta\\\\\n",
    "\\rightarrow \\hat{\\beta} = (X^TX)^{-1}X^T\\beta\n",
    "$$\n",
    "\n",
    "To avoid computing $(X^T X)^{-1}$ directly, X is decomposed into $X = QR$ via QR-decomposition. The solutions become:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = R^{-1} Q^T y\\\\\n",
    "\\hat{y} = QQ^T y\n",
    "$$\n",
    "\n",
    "Because R is upper triangular, it's easier to invert.\n",
    "This is implemented in the gs_solver method below.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To check for collinearity between predictors, the variance inflation factor is calculated as:\n",
    "\n",
    "$$\n",
    "VIF(\\hat{\\beta}_j) = \\frac{1}{1-R^2_j}\n",
    "$$\n",
    "\n",
    "\n",
    "where $R^2_j$ is the RSS of a regression of $x_i$ on all of the other predictors. The rule of thumb is a VIF larger than 10 is an indicator for a collinear predictor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class linear_regression(object):\n",
    "    def __init__(\n",
    "            self,\n",
    "            predictors,\n",
    "            X,\n",
    "            Y,\n",
    "            X_test,\n",
    "            Y_test,\n",
    "            standardize=True,\n",
    "            intercept=True,\n",
    "            weighted=False):\n",
    "        '''\n",
    "            Initalize linear regression object\n",
    "            with the dataset\n",
    "            X = (P+1/P, N) dim matrix\n",
    "            Y = N dim vector\n",
    "            input: Standardize [bool](whiten data)\n",
    "            intercept [bool](Include intercept in model)\n",
    "        '''\n",
    "        if standardize and intercept:\n",
    "            X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "            X_test = (X_test - np.mean(X_test, axis=0)) / np.std(X_test, axis=0)\n",
    "            X = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "            X_test = np.hstack((X_test, np.ones((X_test.shape[0], 1))))\n",
    "            predictors.append('intercept')\n",
    "        elif standardize and not intercept:\n",
    "            X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n",
    "            X_test = (X_test - np.mean(X_test, axis=0)) / np.std(X_test, axis=0)\n",
    "            Y = (Y - np.mean(Y, axis=0)) / np.std(Y, axis=0)\n",
    "        elif not standardize and intercept:\n",
    "            X = np.hstack((X, np.ones((X.shape[0], 1))))\n",
    "            predictors.append('intercept')\n",
    "        # initalized\n",
    "        self.predictors = predictors\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.X_test = X_test\n",
    "        self.Y_test = Y_test\n",
    "\n",
    "\n",
    "    def solve(self, solver='Simple', weighted=False, verbose = True):\n",
    "        '''\n",
    "            Solve regression directly or\n",
    "            with Successive Gram-Schmidt Orthonormalization\n",
    "            solver:  \"Simple\", 'gs' [str] (solution method)\n",
    "            weighted [bool](Weighted OLS)\n",
    "            returns a dataframe with\n",
    "        '''\n",
    "        if solver == 'Simple':\n",
    "            ret = self.solve_simple()\n",
    "        elif solver == 'gs':\n",
    "            ret = gs_solver()\n",
    "        RSS = self._RSS()\n",
    "        rsq = self.r_squared()\n",
    "        self.z_score = self.zscore()\n",
    "        if verbose:\n",
    "            print(f\"***** {solver} Least-Squares Estimate\")\n",
    "            print(\n",
    "                '{:<10.10s}{:<10.8s}{:<10.8s}{:<10.8s}'.format(\n",
    "                    \"Predictor\",\n",
    "                    \"Coef.\",\n",
    "                    \"Std. err.\",\n",
    "                    \"Z-score\"))\n",
    "\n",
    "            dash = '-' * 40\n",
    "            print(dash)\n",
    "            for i in range(len(self.predictors)):\n",
    "                print(\n",
    "                    '{:<10.8s}{:>10.3f}{:>10.3f}{:>10.3f}'.format(\n",
    "                        self.predictors[i],\n",
    "                        self.beta[i][0],\n",
    "                        self.beta[i][0],\n",
    "                        self.z_score[i]))\n",
    "            print(f\"***** R^2: {rsq}\")\n",
    "\n",
    "    def solve_simple(self, weighted=False):\n",
    "        '''\n",
    "            Direct least-squares solution.\n",
    "            b = (XtX)^-1XTY\n",
    "            yhat = xb\n",
    "        '''\n",
    "        self.invxtx = np.linalg.inv(self.X.T @ self.X)\n",
    "        beta = self.invxtx @ self.X.T @ self.Y\n",
    "        y_hat = self.X @ beta\n",
    "        self.beta = beta\n",
    "        self.y_hat = y_hat\n",
    "        return beta, y_hat\n",
    "\n",
    "    def gs_solver(self, weighted=False):\n",
    "        '''\n",
    "            Gram-Schmidt Orthogonalization:\n",
    "            using QR decomposition\n",
    "            Note: QR decomp required Np^2 operations\n",
    "            R - Upper traingular matrix\n",
    "            Q - Note: np.linalg.qr is doing the heavy lifting\n",
    "        '''\n",
    "        Q, R = np.linalg.qr(self.X)\n",
    "        self.beta = np.linalg.inv(R) @ Q.T @ self.Y\n",
    "        self.y_hat = Q.T @ Q @ self.U\n",
    "        return self.beta, self.y_hat\n",
    "\n",
    "    def zscore(self, weighted=False):\n",
    "        '''\n",
    "            Z-score\n",
    "            For the jth predictor\n",
    "            z_j = beta_hat_j / (sqrt(var * v_j))\n",
    "            where v = (X.T*X)_jj\n",
    "        '''\n",
    "        v = np.diag(self.invxtx)\n",
    "        var = self._var()\n",
    "        return np.ravel(self.beta)/(np.sqrt(var)*np.sqrt(v))\n",
    "\n",
    "    def _RSS(self, weighted=False):\n",
    "        '''\n",
    "            Multivariate RSS Calculation\n",
    "        '''\n",
    "        self.rss = None\n",
    "        err = self.Y - self.y_hat\n",
    "        if weighted:\n",
    "            self.rss = np.trace(err.T @ np.cov(err) @ err)\n",
    "        else:\n",
    "            self.rss = np.trace(err.T @ err)\n",
    "        return self.rss\n",
    "\n",
    "    def r_squared(self):\n",
    "        '''\n",
    "            Multivariate RSS Calculation\n",
    "        '''\n",
    "        if self.rss:\n",
    "            tss = np.sum((self.Y - np.mean(self.Y))**2)\n",
    "            return 1 - self.rss/tss\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def _var(self, weighted=False):\n",
    "        '''\n",
    "            Returns an unbiased estimate of the sample variance sigma^2\n",
    "            sigma^1 = 1/(N-p-1) * MSE\n",
    "        '''\n",
    "        N, p = self.X.shape\n",
    "        return 1 / (N - p - 1) * np.sum((self.Y - self.y_hat)**2)\n",
    "\n",
    "\n",
    "    def pred_error(self, beta):\n",
    "        '''\n",
    "            Returns the MSE of an estimate\n",
    "        '''\n",
    "        y_pred = self.X_test @ beta\n",
    "        return np.sum((y_pred - self.Y_test)**2)\n",
    "\n",
    "\n",
    "    def backwards_stepwise_selection(self, plot=False,fname=None):\n",
    "        '''\n",
    "            returns a list of variables dropped during each iterations\n",
    "        '''\n",
    "        import copy\n",
    "        #regress on the full model, then drop the predictor with the smallest\n",
    "        #z-score\n",
    "        x_prev, y_prev = self.X.copy(), self.Y.copy()\n",
    "        x_test_prev  = self.X_test.copy()\n",
    "        pred_prev = self.predictors.copy()\n",
    "        rssarr, p_dropped = [], []\n",
    "        prederr  = []\n",
    "        for i in range(len(self.predictors)-1):\n",
    "            self.solve(verbose = False)\n",
    "            min_idx = np.argmin(np.abs(self.z_score))\n",
    "            p_dropped.append(self.predictors[min_idx])\n",
    "            rssarr.append(self.rss)\n",
    "            prederr.append(self.pred_error(self.beta))\n",
    "            #delete column\n",
    "            self.X = np.delete(self.X,min_idx,axis=1)\n",
    "            self.X_test = np.delete(self.X_test,min_idx,axis=1)\n",
    "            self.predictors = np.delete(self.predictors,min_idx,axis=0)\n",
    "\n",
    "        self.X = x_prev\n",
    "        self.Y = y_prev\n",
    "        self.X_test = x_test_prev\n",
    "        self.predictors = pred_prev\n",
    "\n",
    "        return p_dropped, prederr\n",
    "\n",
    "    \n",
    "    def _variance_inflation_factor(self):\n",
    "        '''\n",
    "            Calculate the Variance inflation factor\n",
    "            (1/(1-RSS_i)) where i is the ith feature\n",
    "            regressed on the remaining features in the dataset\n",
    "            Outputs a table of the variance inflaction factor\n",
    "            to stdout\n",
    "        '''\n",
    "        x_test_prev  = self.X_test.copy()\n",
    "        self.X = np.delete(self.X,len(self.predictors)-1,axis=1)\n",
    "        x_prev, y_prev = self.X.copy(), self.Y.copy()\n",
    "        #drop the predictor column\n",
    "        vif = []\n",
    "        for i in range(len(self.predictors)-1):\n",
    "            self.Y = self.X[:,i].copy().reshape((-1,1))\n",
    "            self.X = np.delete(self.X,i,axis=1)\n",
    "            self.solve(verbose = False)\n",
    "            rs = self.r_squared()\n",
    "            vif.append(1.0/(1.0-rs))\n",
    "            self.X = x_prev\n",
    "        #reset dataset \n",
    "        self.X = x_prev\n",
    "        self.Y = y_prev\n",
    "        \n",
    "        print(\"Variance Inflation factor\")\n",
    "        for i in range(len(self.predictors)-1):\n",
    "            print('{:<10.8s}{:>10.3f}'.format(\n",
    "                self.predictors[i],\n",
    "                vif[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lcavol</th>\n",
       "      <th>lweight</th>\n",
       "      <th>age</th>\n",
       "      <th>lbph</th>\n",
       "      <th>svi</th>\n",
       "      <th>lcp</th>\n",
       "      <th>gleason</th>\n",
       "      <th>pgg45</th>\n",
       "      <th>lpsa</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.579818</td>\n",
       "      <td>2.769459</td>\n",
       "      <td>50</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.430783</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.994252</td>\n",
       "      <td>3.319626</td>\n",
       "      <td>58</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162519</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.510826</td>\n",
       "      <td>2.691243</td>\n",
       "      <td>74</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>-0.162519</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.203973</td>\n",
       "      <td>3.282789</td>\n",
       "      <td>58</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.162519</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.751416</td>\n",
       "      <td>3.432373</td>\n",
       "      <td>62</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.386294</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.371564</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lcavol   lweight  age      lbph  svi       lcp  gleason  pgg45      lpsa  \\\n",
       "1 -0.579818  2.769459   50 -1.386294    0 -1.386294        6      0 -0.430783   \n",
       "2 -0.994252  3.319626   58 -1.386294    0 -1.386294        6      0 -0.162519   \n",
       "3 -0.510826  2.691243   74 -1.386294    0 -1.386294        7     20 -0.162519   \n",
       "4 -1.203973  3.282789   58 -1.386294    0 -1.386294        6      0 -0.162519   \n",
       "5  0.751416  3.432373   62 -1.386294    0 -1.386294        6      0  0.371564   \n",
       "\n",
       "  train  \n",
       "1     T  \n",
       "2     T  \n",
       "3     T  \n",
       "4     T  \n",
       "5     T  "
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##testing with the prostate dataset\n",
    "df = pd.read_csv('./prostate.data',sep='\\s+')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By peaking at the dataset(above), we see a mixture of catagorical and continious data. Luckly, this dataset is a toy-model, catagorical variables are encoded by an index and the data has been cleaned. Training and testing data has already been labeled which is split in the code-cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split train and test\n",
    "df_train = df.loc[df['train'] == 'T']\n",
    "df_test = df.loc[df['train'] == 'F']\n",
    "#drop train column\n",
    "df_train = df_train.drop(['train'],axis=1)\n",
    "df_test = df_test.drop(['train'],axis=1)\n",
    "x_train = df_train[['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45']].to_numpy()\n",
    "y_train = df_train[['lpsa']].to_numpy()\n",
    "x_test = df_test[['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45']].to_numpy()\n",
    "y_test = df_test[['lpsa']].to_numpy()\n",
    "predictors = ['lcavol', 'lweight', 'age', 'lbph', 'svi', 'lcp', 'gleason', 'pgg45']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot correlations between all predictors\n",
    "#grr = pd.plotting.scatter_matrix(df, figsize=(15, 15), marker='o',\n",
    "#                                 hist_kwds={'bins': 20}, s=60, alpha=.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Simple Least-Squares Estimate\n",
      "Predictor Coef.     Std. err  Z-score   \n",
      "----------------------------------------\n",
      "lcavol         0.711     0.711     5.320\n",
      "lweight        0.290     0.290     2.727\n",
      "age           -0.141    -0.141    -1.384\n",
      "lbph           0.210     0.210     2.038\n",
      "svi            0.307     0.307     2.448\n",
      "lcp           -0.287    -0.287    -1.851\n",
      "gleason       -0.021    -0.021    -0.145\n",
      "pgg45          0.275     0.275     1.723\n",
      "intercep       2.452     2.452    27.938\n",
      "***** R^2: 0.6943711796768238\n"
     ]
    }
   ],
   "source": [
    "fh = linear_regression(predictors, x_train, y_train, x_test, y_test, standardize = True, intercept=True)\n",
    "fh.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gleason', 'age', 'lcp', 'pgg45', 'lbph', 'svi', 'lweight', 'lcavol']"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_dropped, prederr = fh.backwards_stepwise_selection()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "plt.plot(np.arange(len(prederr)),prederr)\n",
    "plt.ylim(0,20)\n",
    "plt.xlabel(\"Predictor dropped\")\n",
    "plt.ylabel(\"MSE\")\n",
    "p_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Inflation factor\n",
      "lcavol         2.318\n",
      "lweight        1.472\n",
      "age            1.357\n",
      "lbph           1.383\n",
      "svi            2.045\n",
      "lcp            3.117\n",
      "gleason        2.644\n",
      "pgg45          3.313\n"
     ]
    }
   ],
   "source": [
    "fh._variance_inflation_factor()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
